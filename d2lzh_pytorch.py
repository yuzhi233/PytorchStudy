import torch
from IPython import display
from matplotlib import pyplot as plt
import numpy as np
import random
import torchvision
import torchvision.transforms as transforms
import sys
from torch import nn
import time
import torch.nn.functional as F

#è¿™ä¸ªåŒ…æ˜¯æ•²ä»£ç è¿‡ç¨‹ä¸­æ”¶é›†çš„    ä¹Ÿæ˜¯ä¹¦ä¸Šå†™çš„


# çº¿æ€§å›å½’çš„çŸ¢é‡è®¡ç®—è¡¨è¾¾å¼
def linreg(X, w, b):  # æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzh_pytorchåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
    return torch.mm(X, w) + b


def squared_loss(y_hat, y):  # æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzh_pytorchåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
    # æ³¨æ„è¿™é‡Œè¿”å›çš„æ˜¯å‘é‡, å¦å¤–, pytorché‡Œçš„MSELosså¹¶æ²¡æœ‰é™¤ä»¥ 2
    return (y_hat - y.view(y_hat.size())) ** 2 / 2





def sgd(params, lr, batch_size):  # æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzh_pytorchåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
    for param in params:
        param.data -= lr * param.grad / batch_size # æ³¨æ„è¿™é‡Œæ›´æ”¹paramæ—¶ç”¨çš„param.data å› ä¸ºä½ çš„æ›´æ–°å‚æ•°çš„æ“ä½œä¸èƒ½ä¿å­˜åœ¨è®¡ç®—å›¾ä¸­



def use_svg_display():
    # ç”¨çŸ¢é‡å›¾æ˜¾ç¤º
    display.set_matplotlib_formats('svg')

def set_figsize(figsize=(3.5, 2.5)):
    use_svg_display()
    # è®¾ç½®å›¾çš„å°ºå¯¸
    plt.rcParams['figure.figsize'] = figsize


# æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzhåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)  # æ ·æœ¬çš„è¯»å–é¡ºåºæ˜¯éšæœºçš„
    for i in range(0, num_examples, batch_size):
        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) # æœ€åä¸€æ¬¡å¯èƒ½ä¸è¶³ä¸€ä¸ªbatch
        yield  features.index_select(0, j), labels.index_select(0, j)


# Fashion-MNISTä¸­ä¸€å…±åŒ…æ‹¬äº†10ä¸ªç±»åˆ«ï¼Œåˆ†åˆ«ä¸ºt-shirtï¼ˆTæ¤ï¼‰ã€trouserï¼ˆè£¤å­ï¼‰ã€
# pulloverï¼ˆå¥—è¡«ï¼‰ã€dressï¼ˆè¿è¡£è£™ï¼‰ã€coatï¼ˆå¤–å¥—ï¼‰ã€sandalï¼ˆå‡‰é‹ï¼‰ã€shirtï¼ˆè¡¬è¡«ï¼‰ã€sneakerï¼ˆè¿åŠ¨é‹ï¼‰ã€bagï¼ˆåŒ…ï¼‰å’Œankle bootï¼ˆçŸ­é´ï¼‰ã€‚
# ä»¥ä¸‹å‡½æ•°å¯ä»¥å°†æ•°å€¼æ ‡ç­¾è½¬æˆç›¸åº”çš„æ–‡æœ¬æ ‡ç­¾
def get_fashion_mnist_labels(labels):
    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
    return [text_labels[int(i)] for i in labels]





# ä»¥ä¸‹å‡½æ•°å¯ä»¥åœ¨ä¸€è¡Œé‡Œç”»å‡ºå¤šå¼ å›¾åƒå’Œå¯¹åº”æ ‡ç­¾çš„å‡½æ•°
def show_fashion_mnist(images, labels):#ä¼ å…¥çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ æ•°æ®é›†é‡Œçš„å›¾åƒ(åˆ—è¡¨çš„å½¢å¼)  ç¬¬äºŒä¸ªå‚æ•°æ˜¯æ ‡ç­¾ï¼ˆåˆ—è¡¨ï¼‰
    use_svg_display()
    # è¿™é‡Œçš„_è¡¨ç¤ºæˆ‘ä»¬å¿½ç•¥ï¼ˆä¸ä½¿ç”¨ï¼‰çš„å˜é‡
    _,figs  = plt.subplots(1, len(images), figsize=(12, 12))#1è¡Œ lenï¼ˆimagesï¼‰åˆ—
    for f, img, lbl in zip(figs, images, labels):#æ‰“åŒ…æˆä¸€ä¸ªå¯è¿­ä»£åºåˆ—ï¼ˆfig,image,label)
        img = img.view((28, 28)). numpy()#imgæ˜¯ä»æ•°æ®é›†ï¼ˆæ•°æ®é›†æ˜¯3ç»´åº¦çš„ï¼‰æ‹¿å‡ºæ¥è¢«è½¬æˆäº†tensor æ˜¯æŒ‰(C,H,W)å‚¨å­˜çš„æ˜¯3ç»´çš„ å›¾åƒæ˜¯2ç»´åº¦çš„ï¼Œè¦å…ˆè½¬æ¢æˆ2ç»´åº¦æ‰èƒ½æ˜¾ç¤º
        f.imshow(img)#æ˜¾ç¤ºå›¾åƒ  è‡³äºä¸ºå•¥æ˜¯è¿™ä¸ªè‰² æˆ‘è¿˜åœ¨ç ”ç©¶ æ„Ÿè§‰æ˜¯é€šé“ä¸å¯¹ï¼Œä½†æ˜¯ç°åº¦å›¾ä¸æ˜¯å°±1é€šé“å— å¤´å¤§
        f.set_title(lbl)#å°†æ¯æ¬¡è¯»å–åˆ°çš„æ ‡ç­¾è®¾ç½®æˆæ ‡é¢˜
        f.axes.get_xaxis().set_visible(False)#ä¸æ˜¾ç¤ºxè½´
        f.axes.get_yaxis().set_visible(False)#ä¸æ˜¾ç¤ºyè½´
    plt.show()




# è·å–å¹¶è¯»å–Fashion-MNISTæ•°æ®é›†
# def load_data_fashion_mnist(batch_size):

#     mnist_train = torchvision.datasets.FashionMNIST(root='./Datasets', train=True, download=True, transform=transforms.ToTensor())
#     mnist_test = torchvision.datasets.FashionMNIST(root='./Datasets', train=False, download=True, transform=transforms.ToTensor())


#     if sys.platform.startswith('win'):
#         num_workers = 0  # 0è¡¨ç¤ºä¸ç”¨é¢å¤–çš„è¿›ç¨‹æ¥åŠ é€Ÿè¯»å–æ•°æ®
#     else:
#         num_workers = 4
#     train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)
#     test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)
#     return train_iter,test_iter


# æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzh_pytorchåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
 # è·å–å¹¶è¯»å–Fashion-MNISTæ•°æ®é›†
def load_data_fashion_mnist(batch_size, resize=None, root='./Datasets'):
    """Download the fashion mnist dataset and then load into memory."""
    trans = []#åˆ›å»ºä¸€ä¸ªç”¨æ¥å‚¨å­˜å°†éœ€è¦çš„transæ“ä½œçš„åˆ—è¡¨

    if resize:#å¦‚æœresizeä¸æ˜¯None
        trans.append(torchvision.transforms.Resize(size=resize))
    trans.append(torchvision.transforms.ToTensor())
    transform = torchvision.transforms.Compose(trans)#Composes several transforms together.è¿™ä¸ªæ–¹æ³•å¯ä»¥å°†å‡ ç§transformæ“ä½œç»„åˆèµ·æ¥


    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)
    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)

    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=0)
    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=0)

    return train_iter, test_iter









#å®šä¹‰netåœ¨æ•°æ®é›†çš„å‡†ç¡®ç‡   ä¸»è¦æ˜¯ä¼ å…¥æµ‹è¯•é›†  å·²ç»è¢«ä¸‹é¢å‡çº§ç‰ˆçš„æµ‹è¯•é›†è¯„ä¼°å‡½æ•°å–ä»£ğŸ‘‡
# def evaluate_accuracy(data_iter,net):
#     acc_sum, n =0.0,0
#     for X,y in data_iter:
#         acc_sum+=(net(X).argmax(dim =1) == y).float().sum().item()
#         n += y.shape[0]#è®°å½•æ€»æ•°é‡ è¿™é‡Œä¸€ä¸ªæ‰¹æ¬¡æ˜¯256ä¸ª
#     return acc_sum/n


#æˆ‘ä»¬åœ¨å¯¹ï¼ˆæµ‹è¯•é›†ï¼‰æ¨¡å‹è¯„ä¼°çš„æ—¶å€™ä¸åº”è¯¥è¿›è¡Œdropoutï¼Œæ‰€ä»¥æˆ‘ä»¬ä¿®æ”¹ä¸€ä¸‹d2lzh_pytorchä¸­çš„evaluate_accuracyå‡½æ•°:
# æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzh_pytorch  æ³¨æ„ï¼šå¦‚æœnetæ˜¯é€šè¿‡sequeticalåˆ›å»º ä¸‹é¢æ–¹æ³•ä¼šæŠ¥é”™ ç”¨ä¸Šé¢çš„ğŸ‘†
# def evaluate_accuracy(data_iter, net):#ç”¨äºè¯„ä¼°æµ‹è¯•é›†å‡†ç¡®ç‡ ç›®çš„æ˜¯è¦å®ç°è¯„ä¼°çš„æ—¶å€™è¦è‡ªåŠ¨å…³é—­dropout netæ˜¯åºåˆ—å®¹å™¨ç”Ÿæˆçš„ä¼šæŠ¥é”™ç”¨ä¸Šé¢é‚£ä¸ª
#     acc_sum, n = 0.0, 0
#     for X, y in data_iter:#ä»data_iterå–å‡ºä¸€ä¸ªbatchçš„Xï¼Œy
#          #å…ˆåˆ¤æ–­ä½ è¿™ä¸ªnetæ˜¯æ€ä¹ˆäº§ç”Ÿçš„æ˜¯ä½ è‡ªå·±æ‰‹å†™çš„è¿˜æ˜¯åˆ©ç”¨pytorchå¿«é€Ÿç”Ÿæˆçš„
#         if isinstance(net, torch.nn.Module):#åˆ¤æ–­netæ˜¯ä¸æ˜¯ç”¨torch.nn.Moduleåˆ›å»ºçš„å®ä¾‹
#             net.eval() # #å¦‚æœæ˜¯ä¸Šé¢æ–¹æ³•åˆ›å»ºçš„ é‚£ä¹ˆå¼€å¯è¯„ä¼°æ¨¡å¼ dropoutå±‚å…¨éƒ¨å…³é—­
#             acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()#åˆ¤æ–­æ­£ç¡®çš„ä¸ªæ•°
#             net.train() # æ”¹å›è®­ç»ƒæ¨¡å¼
#         else: # å¦‚æœæ˜¯æˆ‘ä»¬è‡ªå®šä¹‰çš„æ¨¡å‹
#             if('is_training' in net.__code__.co_varnames): # å¦‚æœæœ‰is_trainingè¿™ä¸ªå‚æ•°
#                 # å°†is_trainingè®¾ç½®æˆFalse
#                 acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() #å…ˆå°†is_trainingè®¾ç½®æˆ False å…³é—­dropout
#             else:#(å½¢å‚)æ²¡æœ‰is_trainingè¿™ä¸ªå‚æ•°
#                 acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()
#         n += y.shape[0]#å…¶å®å°±æ˜¯ç®—äº†ä»¥ä¸‹ä¸€ä¸ªæ‰¹æ¬¡æœ‰å¤šå°‘æ ·æœ¬ æ¯æ¬¡å¾ªç¯ç´¯åŠ ä¸€ä¸‹å‚åŠ è®¡ç®—çš„æ ·æœ¬æ•°
#     return acc_sum / n#åœ¨æ‰€æœ‰æ‰¹æ¬¡å¾ªç¯å  è®¡ç®—å‡†ç¡®ç‡ æ‹¿ å‡†ç¡®çš„ä¸ªæ•°/æ€»ä¸ªæ•°

# å› ä¸ºå·ç§¯ç¥ç»ç½‘ç»œè®¡ç®—æ¯”å¤šå±‚æ„ŸçŸ¥æœºè¦å¤æ‚ï¼Œå»ºè®®ä½¿ç”¨GPUæ¥åŠ é€Ÿè®¡ç®—ã€‚
# å› æ­¤ï¼Œæˆ‘ä»¬å¯¹evaluate_accuracyå‡½æ•°ç•¥ä½œä¿®æ”¹ï¼Œä½¿å…¶æ”¯æŒGPUè®¡ç®—
def evaluate_accuracy_2(data_iter, net,device=None):#ç”¨äºè¯„ä¼°æµ‹è¯•é›†å‡†ç¡®ç‡ ç›®çš„æ˜¯è¦å®ç°è¯„ä¼°çš„æ—¶å€™è¦è‡ªåŠ¨å…³é—­dropout
    if device is None and isinstance(net,torch.nn.Module):#å¦‚æœè®¾å¤‡æ˜¯noneä¸”netæ˜¯ç”±nn.moduleç”Ÿæˆçš„å®ä¾‹ã€‚åˆ™ï¼š
          # å¦‚æœæ²¡æŒ‡å®šdeviceå°±ä½¿ç”¨netçš„device
        device = list(net.parameters())[0].device
    acc_sum, n = 0.0, 0
    with torch.no_grad():#ä¸è¿½è¸ªæ“ä½œ
        for X, y in data_iter:#ä»data_iterå–å‡ºä¸€ä¸ªbatchçš„X,y

          #å…ˆåˆ¤æ–­ä½ è¿™ä¸ªnetæ˜¯æ€ä¹ˆäº§ç”Ÿçš„æ˜¯ä½ è‡ªå·±æ‰‹å†™çš„è¿˜æ˜¯åˆ©ç”¨pytorchå¿«é€Ÿç”Ÿæˆçš„
            if isinstance(net, torch.nn.Module):#åˆ¤æ–­netæ˜¯ä¸æ˜¯ç”¨torch.nn.Moduleåˆ›å»ºçš„å®ä¾‹(åˆ¤æ–­netæ˜¯ä¸æ˜¯åˆ©ç”¨moduleæ¨¡å—æ­å»ºçš„)

                net.eval() # #å¦‚æœæ˜¯ä¸Šé¢æ–¹æ³•åˆ›å»ºçš„ é‚£ä¹ˆå¼€å¯è¯„ä¼°æ¨¡å¼ dropoutå±‚å…¨éƒ¨å…³é—­(å› ä¸ºæˆ‘ä»¬è¦æ˜¯é€šè¿‡moduleæ¨¡å—åˆ›å»ºä¸€ä¸ªæ¨¡å‹æœ‰å¯èƒ½æ·»åŠ äº†dropoutå±‚)
                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()#åˆ¤æ–­æ­£ç¡®çš„ä¸ªæ•°
                net.train() # æ”¹å›è®­ç»ƒæ¨¡å¼
            else: # å¦‚æœæ˜¯æˆ‘ä»¬è‡ªå®šä¹‰çš„æ¨¡å‹    elseä¸‹é¢çš„è¿™æ®µä¸»è¦æ˜¯ç”¨äº3.13èŠ‚æˆ‘ä»¬è‡ªå®šä¹‰å¸¦dropoutçš„æ¨¡å‹ï¼Œè®¡ç®—å‡†ç¡®ç‡çš„ä»¥åä¸ä¼šç”¨åˆ° ä¸è€ƒè™‘GPU
                if('is_training' in net.__code__.co_varnames): # å¦‚æœæœ‰is_trainingè¿™ä¸ªå‚æ•°
                # å°†is_trainingè®¾ç½®æˆFalse
                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() #å…ˆå°†is_trainingè®¾ç½®æˆ False å…³é—­dropout
                else:#(å½¢å‚)æ²¡æœ‰is_trainingè¿™ä¸ªå‚æ•°
                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()
            n += y.shape[0]#å…¶å®å°±æ˜¯ç®—äº†ä»¥ä¸‹ä¸€ä¸ªæ‰¹æ¬¡æœ‰å¤šå°‘æ ·æœ¬ æ¯æ¬¡å¾ªç¯ç´¯åŠ ä¸€ä¸‹å‚åŠ è®¡ç®—çš„æ ·æœ¬æ•°
    return acc_sum / n#åœ¨æ‰€æœ‰æ‰¹æ¬¡å¾ªç¯å  è®¡ç®—å‡†ç¡®ç‡ æ‹¿ å‡†ç¡®çš„ä¸ªæ•°/æ€»ä¸ªæ•°





#æ¨¡å‹è®­ç»ƒå¹¶è®¡ç®—å‡†ç¡®ç‡
#å‚æ•°1 :ä¼ å…¥çš„æ˜¯æˆ‘ä»¬å®šä¹‰çš„net
#å‚æ•°2 :ä¼ å…¥çš„è®­ç»ƒé›†æ•°æ®ç”Ÿæˆå™¨
#å‚æ•°3 :ä¼ å…¥çš„æµ‹è¯•é›†æ•°æ®ç”Ÿæˆå™¨
#å‚æ•°4ï¼šä¼ å…¥å®šä¹‰çš„æŸå¤±å‡½æ•°æ˜¯å“ªä¸€ç§
#å‚æ•°5 :ä¼ å…¥ä¸€ä¸ªbatchå¤§å°
#å‚æ•°6 :æ€»å…±éå†å‡ æ¬¡å…¨æ ·æœ¬
#å‚æ•°7 :ä¼ å…¥å‚æ•° åˆ—è¡¨å½¢å¼[w1,b1..]
#å‚æ•°8 :å­¦ä¹ ç‡
#å‚æ•°9 :ä¼˜åŒ–å™¨
def train_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,params =None,lr=None,optimizer=None):#ch3è¡¨ç¤ºé€‚ç”¨ç¬¬ä¸‰ç« 

    for epoch in range(num_epochs):
        train_l_sum,train_acc_sum,n = 0.0,0.0,0
        for X,y in train_iter:
            y_hat =net(X)
            l =loss(y_hat,y).sum()

            #æ¢¯åº¦æ¸…é›¶
            if optimizer is not None:#å¦‚æœå®šä¹‰äº†ä¼˜åŒ–ç®—æ³•
                optimizer.zero_grad()#ä¼˜åŒ–ç®—æ³•çš„å‚æ•°æ¢¯åº¦æ¸…é›¶
            elif params is not None and params[0].grad is not None:#å¦‚æœå®šä¹‰äº†å‚æ•°
                for param in params:#æ¯ä¸ªå‚æ•°æ¢¯åº¦æ¸…é›¶
                    param.grad.data.zero_()

            #åå‘ä¼ æ’­
            l.backward()

            #å‚æ•°æ›´æ–°
            if optimizer is None:#å¦‚æœæ²¡æœ‰è‡ªå·±å®šä¹‰ä¼˜åŒ–ç®—æ³•
                sgd(params,lr,batch_size)#æ›´æ–°å‚æ•°å°±æŒ‰å°æ‰¹é‡æ¢¯åº¦ä¸‹é™çš„æ–¹æ³•æ›´æ–°å‚æ•°
            else:
                optimizer.step()#å®šä¹‰äº†ä¼˜åŒ–ç®—æ³•å°±stepè‡ªåŠ¨æ›´æ–°


            train_l_sum += l.item()#æ¯ä¸€æ‰¹æ¬¡æ ·æœ¬è®¡ç®—å®Œè®°å½•ä¸‹loss è¿™é‡Œçš„lossæ˜¯è®­ç»ƒé›†çš„loss  læ˜¯tensorç±»å‹ ,item()æ˜¯è½¬æˆpython number

            train_acc_sum +=(y_hat.argmax(dim=1) == y).sum().item()# è®¡ç®—æ¯ä¸ªæ‰¹æ¬¡y_hatä¸­é¢„æµ‹æ­£ç¡®çš„ä¸ªæ•°å¹¶ç´¯åŠ 

            n+=y.shape[0]#shapeçš„ç»“æœæ˜¯ä¸€ä¸ªæ‰¹æ¬¡ä¸­æ ·æœ¬çš„ä¸ªæ•°ä¹Ÿå°±æ˜¯batchsize #shapeæ˜¯torch.size[256]ï¼ŒåŠ ä¸Šç´¢å¼•æ‰èƒ½æŠŠå€¼æ‹¿å‡ºæ¥

        test_acc =evaluate_accuracy(test_iter,net)
        print('epoch{},loss{:.4f},train acc{:.3f},test acc{:.3f}'.format(epoch+1,train_l_sum/n,train_acc_sum/n,test_acc))



# æˆ‘ä»¬åŒæ ·å¯¹3.6èŠ‚ä¸­å®šä¹‰çš„train_ch3å‡½æ•°ç•¥ä½œä¿®æ”¹ï¼Œç¡®ä¿è®¡ç®—ä½¿ç”¨çš„æ•°æ®å’Œæ¨¡å‹åŒåœ¨å†…å­˜æˆ–æ˜¾å­˜ä¸Šã€‚
  # æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzh_pytorchåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
def train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs):
    net = net.to(device)#å°†æ¨¡å‹æ·»åŠ åˆ°è®¾å¤‡ä¸Š å¯èƒ½æ˜¯GPUå¯èƒ½æ˜¯CPU
    print("training on ", device)#æ˜¾ç¤ºæ˜¯åœ¨å“ªè®­ç»ƒçš„
    loss = torch.nn.CrossEntropyLoss()#ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°
    for epoch in range(num_epochs):#å‡ ä¸ªè¿­ä»£å‘¨æœŸ å¼€å§‹è¿­ä»£
        #å®šä¹‰ è®­ç»ƒé›†æŸå¤±å’Œ,è®­ç»ƒé›†å‡†ç¡®æ€»æ•°ï¼Œæ€»æ ·æœ¬æ•°n,å‡ ä¸ªbatch,å¼€å§‹æ—¶é—´
        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()
        for X, y in train_iter:#ä¸€ä¸ªæ‰¹æ¬¡ä¸­(æ¯”å¦‚ä¸€ä¸ªbatch_sizeæ˜¯256)
            X = X.to(device)#å…ˆå°†éœ€è¦è®¡ç®—çš„æ·»åŠ åˆ° è®¾å¤‡
            y = y.to(device)#åŒä¸Š

            y_hat = net(X)#è®¡ç®—æ¨¡å‹é¢„æµ‹å€¼y_hat---->

            l = loss(y_hat, y)#è®¡ç®—æŸå¤±ï¼ˆåˆ©ç”¨å‰é¢å®šä¹‰çš„äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼‰

            optimizer.zero_grad()#ä¼˜åŒ–å™¨æ¢¯åº¦æ¸…0
            l.backward()#è¯¯å·®åå‘ä¼ æ’­
            optimizer.step()#æ¢¯åº¦æ›´æ–°
            train_l_sum += l.cpu().item()#è®¡ç®—å¾—åˆ°çš„è¯¯å·®å¯èƒ½å†GPUä¸Šå…ˆç§»åŠ¨åˆ°CPUè½¬æˆpytonæ•°å­—
            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()#è®­ç»ƒé›†æ­£ç¡®æ€»æ•°
            n += y.shape[0]
            batch_count += 1

        #åœ¨è®­ç»ƒé›†è¿­ä»£å®Œç”¨æµ‹è¯•é›†è¿›è¡Œæ¨¡å‹è¯„ä¼°
        test_acc = evaluate_accuracy_2(test_iter, net)
        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'
              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))









 # å¯¹xçš„å½¢çŠ¶è½¬æ¢çš„è¿™ä¸ªåŠŸèƒ½è‡ªå®šä¹‰ä¸€ä¸ªFlattenLayerå¹¶è®°å½•åœ¨d2lzh_pytorchä¸­æ–¹ä¾¿åé¢ä½¿ç”¨ã€‚
# æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzh_pytorchåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
class FlattenLayer(nn.Module):
    def __init__(self):
        super(FlattenLayer, self).__init__()
    def forward(self, x): # x shape: (batch, *, *, ...)
        return x.view(x.shape[0], -1)




# æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzh_pytorchåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
        #ç”»å›¾ yè½´æ˜¯å¯¹æ•°å°ºåº¦çš„
def semilogy(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,legend=None, figsize=(3.5, 2.5)):
    set_figsize(figsize)
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.semilogy(x_vals, y_vals)
    if x2_vals and y2_vals :
        plt.semilogy(x2_vals, y2_vals, linestyle=':')
        plt.legend(legend)




#å‡½æ•°ä½œç”¨:è¾“å…¥å›¾åƒå¼ é‡çš„äºŒç»´æ•°ç»„ å’Œå·ç§¯æ ¸å¤§å° è¿”å›äºŒç»´æ•°ç»„ä¸å·ç§¯æ ¸ è¿›è¡Œå·ç§¯åçš„å›¾åƒå¼ é‡æ•°ç»„   è¿™é‡Œæ­¥é•¿åº”è¯¥é»˜è®¤æ˜¯1 padding =0
def corr2d(X,K):#X---éœ€è¦è¿›è¡Œå·ç§¯çš„äºŒç»´æ•°ç»„   K------å·ç§¯æ ¸å¼ é‡æ•°ç»„
    h,w =K.shape
    Y = torch.zeros((X.shape[0]-h+1,X.shape[1]-w+1))#åˆå§‹åŒ–y  Yæ˜¯å·ç§¯åçš„äºŒç»´tensor
    for i in range(Y.shape[0]):#éå†Yæ¯ä¸ªå…ƒç´  (å¡«ç»è¿‡å·ç§¯åçš„å€¼)
        for j in range(Y.shape[1]):
            Y[i,j]=(X[i:i+h,j:j+w]*K).sum()#æˆªå–äº†ä¸€å°å—åšå·ç§¯ è¿™é‡Œ*K æ˜¯å¹¿æ’­è¿ç®— å¯¹äºå…ƒç´ ç›¸ä¹˜ ç®—å‡ºäº†ä¸€æ¬¡å·ç§¯çš„çš„ç»“æœ

    return Y






class GlobalAvgPool2d(nn.Module):
    # # å…¨å±€å¹³å‡æ± åŒ–å±‚å¯é€šè¿‡å°†æ± åŒ–çª—å£å½¢çŠ¶è®¾ç½®æˆè¾“å…¥çš„é«˜å’Œå®½å®ç° ç­‰äºä¸€å¼ å›¾ å…¨å±€æ± åŒ–æˆä¸€ä¸ªæ•°
    def __init__(self):
        super(GlobalAvgPool2d,self).__init__()

    def forward(self,x):
        return F.avg_pool2d(x,kernel_size =x.size()[2:])#å›¾åƒæ˜¯æŒ‰ æ ·æœ¬æ•° é€šé“æ•° H W  è¿™é‡Œå–çš„æ˜¯HW